# Overview
This project is focused on reading in output from the X-ray flourensene (XRF) analyzer located at the large lakes obsveratory as well as standardizing and calibrating it. This has been done by hand in the past taking up to ten hours per week, and this project is the automation of that process. It involves reading in dozens up to hundreds of output files, as well as the corresponding control samples recieved from the national institute of standards and technology (NIST) to combine and calibrate the files into a single output that is both accurate and easy to read.


# UROP
The university of Minnesota, Duluth offers a program titled 'Undergraduate Research Opprotunity Program'. This program involves undergraduate students creating and proposing potential projects from any discipline. For my project, I worked with Aaron Lingwall, the lab coordinator of the XRF Analyzer, to help automate the process of standardizing and calibrating the output from that analyzer. There was also another utility program to help concentrate the output files.


# Background
The  XRF analyzer, simply, is a machine to find out the concentration of different elements in rock/mud/dirt of rock cores ranging from 1-180 cm. A rock core is a cylindrical shaped sample taken from deep within the earth. These cores are then seperated, with one being archive and one being destroyable. The XRF Lab is sent the archived half, as the XRF analyzer does not physically impact the sample. These cores are part of a large sample often dozens or hundreds of meters long. Because of the size of some samples, it can take up to months to fully complete a sample. The x-ray tubes of the analyzer are incredibly sensitive, so degredation is quick and very impactful to the samples. To counteract this, known samples from the NIST are run every week as controls. These controls can then be measured against eachother to get the conversion rate from one week to the other. The XRF analyzer directly outputs XRF counts, its recorded amount of signals recieved for each element. XRF counts on their own are incomparable and must be converted to percent concentration using these control samples. 


# Project
I completed this project using Java in the netbeans IDE. At the time, I was most comfortable and confident in Java. The XRF output was as a .xls file, meaning it can be read in as a .csv and easily parsed in Java. Each file contains a list of every element present in that specific part of the sample. Not all samples contain the same elements so these are a major source of conflict between the files. So a master list of all possible elements was created and populated using the read in data. Once all files were read into the master list, each element was checked, and if every sample did not contain that element, it was removed from the list. This created a standard format that is easily readable and contains no meaningless columns. After this, the master list is output as a .xls file. This file is then read into a different program, along with the folder destination of the NIST control samples. These samples are read in, with the date they were run also being saved. The first sample in the master list is used as the control, with the rest of the list having to calibrate to its level. Each sample's run date is recorded and used to compare the cooresponding NIST control file to the initial NIST file to create a conversion. This conversion is then applied and the final results saved in a seperate .xlsx file. This creates an comparable list of easily readable data.
